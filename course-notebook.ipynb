{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 01: Introduction to Theano\n",
    "\n",
    "Hi, Theano is a Python library for fast numerical computation to aid in the development of deep learning models.\n",
    "\n",
    "At its heart, Theano is a compiler for mathematical expressions in Python. It knows how to take your structures and turn them into very efficient code that uses NumPy and efficient native libraries to run as fast as possible on CPUs or GPUs.\n",
    "\n",
    "The actual syntax of Theano expressions is symbolic, which can be off-putting to beginners used to normal software development. Specifically, expression are defined in the abstract sense, compiled and later actually used to make calculations.\n",
    "\n",
    "In this lesson, your goal is to install Theano and write a small example that demonstrates the symbolic nature of Theano programs.\n",
    "\n",
    "For example, you can install Theano using pip as follows:\n",
    "```\n",
    "sudo pip install Theano\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A small example of a Theano program that you can use as a starting point is listed below:\n",
    "\n",
    "import theano\n",
    "from theano import tensor\n",
    "# declare two symbolic floating-point scalars\n",
    "a = tensor.dscalar()\n",
    "b = tensor.dscalar()\n",
    "# create a simple expression\n",
    "c = a + b\n",
    "# convert the expression into a callable object that takes (a,b)\n",
    "# values as input and computes a value for c\n",
    "f = theano.function([a,b], c)\n",
    "# bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\n",
    "result = f(1.5, 2.5)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 02:  Introduction to TensorFlow\n",
    "\n",
    "Hi, TensorFlow is a Python library for fast numerical computing created and released by Google. Like Theano, TensorFlow is intended to be used to develop deep learning models.\n",
    "\n",
    "With the backing of Google, perhaps used in some of its production systems and used by the Google DeepMind research group, it is a platform that we cannot ignore.\n",
    "\n",
    "Unlike Theano, TensorFlow does have more of a production focus with a capability to run on CPUs, GPUs and even very large clusters.\n",
    "\n",
    "In this lesson, your goal is to install TensorFlow become familiar with the syntax of the symbolic expressions used in TensorFlow programs.\n",
    "\n",
    "For example, you can install TensorFlow using pip. There are many different versions of TensorFlow, specialized for each platform. Select the right version for your platform on the TensorFlow installation webpage.\n",
    "\n",
    "Used method:\n",
    "\n",
    "```\n",
    "source activate root # root default conda env\n",
    "sudo pip install tensorflow  # basic install no CUDA support\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A small example of a TensorFlow program that you can use as a starting point is listed below:\n",
    "\n",
    "import tensorflow as tf\n",
    "# declare two symbolic floating-point scalars\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "# create a simple symbolic expression using the add function\n",
    "add = tf.add(a, b)\n",
    "# bind 1.5 to ' a ' , 2.5 to ' b ' , and evaluate ' c '\n",
    "sess = tf.Session()\n",
    "binding = {a: 1.5, b: 2.5}\n",
    "c = sess.run(add, feed_dict=binding)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 03: Intro to Keras\n",
    "\n",
    "Hi, a difficulty of both Theano and TensorFlow is that it can take a lot of code to create even very simple neural network models.\n",
    "\n",
    "These libraries were designed primarily as a platform for research and development more than for the practical concerns of applied deep learning.\n",
    "\n",
    "The Keras library addresses these concerns by providing a wrapper for both Theano and TensorFlow. It provides a clean and simple API that allows you to define and evaluate deep learning models in just a few lines of code.\n",
    "\n",
    "Because of the ease of use and because it leverages the power of Theano and TensorFlow, Keras is quickly becoming the go-to library for applied deep learning.\n",
    "\n",
    "The focus of Keras is the concept of a model. The life-cycle of a model can be summarized as follows:\n",
    "\n",
    "1. Define your model. Create a Sequential model and add configured layers.\n",
    "\n",
    "2. Compile your model. Specify loss function and optimizers and call the compile() function on the model.\n",
    "\n",
    "3. Fit your model. Train the model on a sample of data by calling the fit() function on the model.\n",
    "\n",
    "4. Make predictions. Use the model to generate predictions on new data by calling functions such as evaluate() or predict() on the model.\n",
    "\n",
    "Your goal for this lesson is to install Keras.\n",
    "\n",
    "For example, you can install Keras using pip:\n",
    "\n",
    "```sudo pip install keras```\n",
    "\n",
    "Start to familiarize yourself with the Keras library ready for the upcoming lessons where we will implement our first model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 04: Crash Course in Multi-Layer Perceptrons\n",
    "\n",
    "Hi, artificial neural networks are a fascinating area of study, although they can be intimidating when just getting started.\n",
    "\n",
    "The field of artificial neural networks is often just called neural networks or multi-layer Perceptrons after perhaps the most useful type of neural network.\n",
    "\n",
    "The building block for neural networks are artificial neurons. These are simple computational units that have weighted input signals and produce an output signal using an activation function.\n",
    "\n",
    "Neurons are arranged into networks of neurons. A row of neurons is called a layer and one network can have multiple layers. The architecture of the neurons in the network is often called the network topology.\n",
    "\n",
    "Once configured, the neural network needs to be trained on your dataset. The classical and still preferred training algorithm for neural networks is called stochastic gradient descent.\n",
    "\n",
    "Your goal for this lesson is to become familiar with neural network terminology.\n",
    "\n",
    "Dig a little deeper into terms like neuron, weights, activation function, learning rate and more.\n",
    "\n",
    "-----\n",
    "\n",
    "See [A Basic Introduction To Neural Networks](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Your First Neural Network in Keras\n",
    "\n",
    "Hi, Keras allows you to develop and evaluate deep learning models in very few lines of code.\n",
    "\n",
    "In this lesson, your goal is to develop your first neural network using the Keras library.\n",
    "\n",
    "Use a standard binary (two-class) classification dataset from the UCI Machine Learning Repository, like the Pima Indians onset of diabetes or the ionosphere datasets.\n",
    "\n",
    "Piece together code to achieve the following:\n",
    "Load your dataset using NumPy or Pandas.\n",
    "Define your neural network model and compile it.\n",
    "Fit your model to the dataset.\n",
    "Estimate the performance of your model on unseen data.\n",
    "To give you a massive head-start below is a complete working example that you can use as a starting point.\n",
    "\n",
    "It assumes that you have downloaded the Pima Indians dataset to your current working directory with the filename pima-indians-diabetes.csv.\n",
    "\n",
    "\n",
    "\n",
    "Afterwards, develop your own model on a different dataset, or adapt this example.\n",
    "\n",
    "Learn more about the Keras API for simple model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 5.2226 - acc: 0.4609     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.9556 - acc: 0.6380     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.7014 - acc: 0.6471     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.6864 - acc: 0.6471     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6777 - acc: 0.6484     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6695 - acc: 0.6484     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6629 - acc: 0.6484     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6563 - acc: 0.6510     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.6520 - acc: 0.6484     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.6508 - acc: 0.6484     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.6497 - acc: 0.6510     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.6458 - acc: 0.6497     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.6456 - acc: 0.6510     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6459 - acc: 0.6510     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6440 - acc: 0.6484     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.6406 - acc: 0.6510     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.6407 - acc: 0.6497     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.6451 - acc: 0.6497     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.6383 - acc: 0.6523     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.6399 - acc: 0.6484     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.6378 - acc: 0.6497     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.6405 - acc: 0.6497     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.6382 - acc: 0.6510     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.6379 - acc: 0.6497     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.6381 - acc: 0.6510     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.6361 - acc: 0.6523     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.6367 - acc: 0.6510     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.6379 - acc: 0.6497     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.6365 - acc: 0.6510     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.6357 - acc: 0.6510     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.6365 - acc: 0.6523     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.6364 - acc: 0.6510     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.6353 - acc: 0.6523     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.6338 - acc: 0.6523     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.6364 - acc: 0.6523     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.6349 - acc: 0.6510     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.6374 - acc: 0.6510     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.6349 - acc: 0.6523     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.6359 - acc: 0.6523     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.6357 - acc: 0.6523     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.6342 - acc: 0.6523     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.6350 - acc: 0.6523     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.6349 - acc: 0.6523     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.6343 - acc: 0.6536     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.6335 - acc: 0.6523     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.6347 - acc: 0.6510     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.6349 - acc: 0.6523     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.6337 - acc: 0.6523     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.6340 - acc: 0.6523     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.6347 - acc: 0.6510     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.6341 - acc: 0.6523     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.6342 - acc: 0.6523     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.6332 - acc: 0.6523     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.6314 - acc: 0.6523     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.6374 - acc: 0.6523     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.6321 - acc: 0.6523     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.6374 - acc: 0.6523     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.6383 - acc: 0.6523     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.6366 - acc: 0.6510     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.6357 - acc: 0.6510     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.6337 - acc: 0.6523     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.6315 - acc: 0.6510     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.6314 - acc: 0.6510     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.6304 - acc: 0.6523     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.6318 - acc: 0.6510     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.6301 - acc: 0.6523     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.6328 - acc: 0.6536     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.6296 - acc: 0.6523     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.6306 - acc: 0.6523     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.6316 - acc: 0.6523     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.6329 - acc: 0.6523     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.6316 - acc: 0.6523     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.6286 - acc: 0.6523     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.6304 - acc: 0.6523     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.6295 - acc: 0.6510     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.6257 - acc: 0.6523     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.6254 - acc: 0.6510     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.6217 - acc: 0.6523     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.6146 - acc: 0.6536     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.6177 - acc: 0.6523     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.6194 - acc: 0.6510     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.6118 - acc: 0.6523     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.6076 - acc: 0.6523     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.6041 - acc: 0.6523     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.6150 - acc: 0.6523     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.6077 - acc: 0.6523     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.6041 - acc: 0.6523     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.6010 - acc: 0.6523     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.6011 - acc: 0.6523     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.6026 - acc: 0.6536     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.5958 - acc: 0.6523     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.6029 - acc: 0.6523     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.6004 - acc: 0.6523     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.5970 - acc: 0.6510     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.5944 - acc: 0.6523     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.5942 - acc: 0.6523     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.5916 - acc: 0.6523     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.5921 - acc: 0.6523     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.5922 - acc: 0.6523     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.5906 - acc: 0.6536     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.5917 - acc: 0.6523     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.5900 - acc: 0.6523     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.5923 - acc: 0.6523     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.5912 - acc: 0.6536     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.5947 - acc: 0.6523     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.5921 - acc: 0.6523     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.5894 - acc: 0.6523     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.5862 - acc: 0.6523     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.5895 - acc: 0.6523     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.5878 - acc: 0.6536     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.5837 - acc: 0.6536     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.5863 - acc: 0.6523     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.5860 - acc: 0.6523     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.5848 - acc: 0.6536     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.5881 - acc: 0.6523     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.5808 - acc: 0.6523     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.5805 - acc: 0.6523     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.5820 - acc: 0.6536     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.5823 - acc: 0.6523     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.5854 - acc: 0.6536     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.5824 - acc: 0.6536     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.5783 - acc: 0.6536     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.5811 - acc: 0.6536     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.5809 - acc: 0.6523     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.5797 - acc: 0.6536     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.5740 - acc: 0.6536     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.5825 - acc: 0.6523     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.5708 - acc: 0.6536     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.5737 - acc: 0.6536     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.5826 - acc: 0.6523     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.5808 - acc: 0.6536     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.5752 - acc: 0.6536     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.5722 - acc: 0.6536     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.5753 - acc: 0.6523     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.5773 - acc: 0.6536     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.5740 - acc: 0.6536     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.5745 - acc: 0.6523     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.5769 - acc: 0.6536     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.5743 - acc: 0.6536     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.5755 - acc: 0.6536     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.5701 - acc: 0.6536     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.5703 - acc: 0.6536     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.5705 - acc: 0.6536     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.5749 - acc: 0.6536     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.5711 - acc: 0.6536     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.5686 - acc: 0.6536     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.5690 - acc: 0.6706     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.5679 - acc: 0.7005     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.5709 - acc: 0.6940     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.5679 - acc: 0.6979     \n",
      " 32/768 [>.............................] - ETA: 0sacc: 69.92%\n"
     ]
    }
   ],
   "source": [
    "# Create first network with Keras\n",
    "# import Keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "# Keras 1.2 doesn't support separate kernel/bias initializer\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "# model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "# model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)\n",
    "# Keras 2+ only:\n",
    "# model.fit(X, Y, epochs=150, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified version: Separates dataset into training and test\n",
    "\n",
    "[Source](https://gist.github.com/codekitchen/ee4ab59e1dbd8024ea88275cb31e3363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[   6.     148.      72.      35.       0.      33.6      0.627   50.       1.   ]\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_10 (Dense)                 (None, 12)            108         dense_input_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 8)             104         dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1)             9           dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "32/68 [=============>................] - ETA: 0sacc: 67.65%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (0.0, 0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create first network with Keras\n",
    "# import Keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load ionosfere dataset\n",
    "            \n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "print len(dataset)\n",
    "print dataset[0]\n",
    "\n",
    "trainX = X[0:700]\n",
    "trainY = Y[0:700]\n",
    "testX = X[700:]\n",
    "testY = Y[700:]\n",
    "len(testX)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "# Keras 1.2 doesn't support separate kernel/bias initializer\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(trainX, trainY, nb_epoch=150, batch_size=10, verbose=0)\n",
    "# Keras 2+ only:\n",
    "# model.fit(X, Y, epochs=150, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(testX, testY)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "predictions = model.predict(testX)\n",
    "predictions = [round(x[0]) for x in predictions]\n",
    "list(zip(testY, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 06: Use Keras Models With Scikit-Learn\n",
    "\n",
    "the scikit-learn library is a general purpose machine learning framework in Python built on top of SciPy.\n",
    "\n",
    "Scikit-learn excels at tasks such as evaluating model performance and optimizing model hyperparameters in just a few lines of code.\n",
    "\n",
    "Keras provides a wrapper class that allows you to use your deep learning models with scikit-learn. For example, an instance of KerasClassifier class in Keras can wrap your deep learning model and be used as an Estimator in scikit-learn.\n",
    "\n",
    "When using the KerasClassifier class, you must specify the name of a function that the class can use to define and compile your model. You can also pass additional parameters to the constructor of the KerasClassifier class that will be passed to the model.fit() call later, like the number of epochs and batch size.\n",
    "\n",
    "In this lesson, your goal is to develop a deep learning model and evaluate it using k-fold cross validation.\n",
    "\n",
    "Learn more about using your Keras deep learning models with scikit-learn on the [Wrappers for the Sciki-Learn API webpage](https://keras.io/scikit-learn-api/).\n",
    "\n",
    "\n",
    "For example, you can define an instance of the KerasClassifier and the custom function to create your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT RUNNABLE\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    ...\n",
    "    # Compile model\n",
    "    model.compile(...)\n",
    "    return model\n",
    "\n",
    "# create classifier for use in scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10)\n",
    "# evaluate model using 10-fold cross validation in scikit-learn\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 07: Plot Model Training History\n",
    "\n",
    "You can learn a lot about neural networks and deep learning models by observing their performance over time during training.\n",
    "\n",
    "Keras provides the capability to register callbacks when training a deep learning model.\n",
    "\n",
    "One of the default callbacks that is registered when training all deep learning models is the History callback. It records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) as well as the loss and accuracy for the validation dataset if one is set.\n",
    "\n",
    "The history object is returned from calls to the fit() function used to train the model. Metrics are stored in a dictionary in the history member of the object returned.\n",
    "\n",
    "Your goal for this lesson is to investigate the history object and create plots of model performance during training.\n",
    "\n",
    "You can learn more about the History object and the [callback API in Keras](https://keras.io/callbacks/?__s=armbrtwyszcbhtwmentx#history).\n",
    "\n",
    "\n",
    "For example, you can print the list of metrics collected by your history object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "history = model.fit(...)\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
